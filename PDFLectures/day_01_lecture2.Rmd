---
title: "Data Wrangling"
output: html_document
---


## Parsing Dates and Times

We have described three main types of vectors: numeric, character, and logical. In data science projects we very often encounter variables that are dates. Although we can represent a date with a string, for example, `September 8, 2021`, once we pick a reference day, referred to as the _epoch_, they can be converted to numbers. Computer languages usually use [January 1, 1970 as the epoch](https://www.quora.com/Why-does-epoch-time-start-with-January-1-1970). So, September 8, 2021 is day 18,878. 

Now how should we represent dates and times when analyzing data in R? We could just use days since the epoch, but then it is almost impossible to interpret. If I tell you it's September 8, 2021, you know what this means immediately. If I tell you it's day 18,878, you will be quite confused. Similar problems arise with times. In this case it gets even more complicated due to time zones. 

For this reason R defines a data type just for dates and times. We can see an example in the [polls data](https://cran.r-project.org/web/packages/dslabs/dslabs.pdf):

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
polls_us_election_2016$startdate %>% head
```

These look like strings. But they are not:

```{r}
class(polls_us_election_2016$startdate)
```

Look at what happens when we convert them to numbers:

```{r}
as.numeric(polls_us_election_2016$startdate) %>% head
```

It turns them into dates since the epoch. 

Plotting functions, such as those in ggplot, are aware of dates. This means that, for example, a scatter plot can use the numeric representation to decide on the position of the point, but include the string in the labels:

```{r}
polls_us_election_2016 %>% filter(pollster == "Ipsos" & state =="U.S.") %>%
  ggplot(aes(startdate, rawpoll_trump)) +
  geom_line()
```

Note in particular that the months are displayed. The tidyverse includes a functionality for dealing with dates through the `lubridate` package. 

```{r, message=FALSE, warning=FALSE}
library(lubridate)
```

We will take a random sample of dates to show some of the useful things one can do:
```{r}
set.seed(2)
dates <- sample(polls_us_election_2016$startdate, 10) %>% sort
dates
```

The functions `year`, `month` and `day` extract those values:

```{r}
data.frame(month = month(dates),
           day = day(dates),
           year = year(dates))
```

We can also extract the month labels:

```{r}
month(dates, label = TRUE)
```


Another useful set of functions are the _parsers_ that convert strings into dates.

```{r}
x <- c(20090101, "2009-01-02", "2009 01 03", "2009-1-4",
       "2009-1, 5", "Created on 2009 1 6", "200901 !!! 07")
ymd(x)
```


A further complication comes from the fact that dates often come in different formats in which the order of year month and day are different. The preferred format is to show year (with all four digits), month (two digits) and then day or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string it will be ordered by date. You can see the function `ymd` returns them in this format.

What if you encouter dates such as "09/01/02"? This could be September 1, 2002 or Janary 2, 2009 or January 9, 2002. 
In these cases examining the entire vector of dates will help you determine what format it is by process of elimination. Once you know, you can make use of the many parsers provided by lubridate.

For example, if the string is

```{r}
x <- "09/01/02"
```

The `ymd` function assumes the first entry is the year the second the month and the third the day so it coverts it to:

```{r}
ymd(x)
```

The `mdy` function assumes the first entry is the month then the day then the year:

```{r}
mdy(x)
```

Lubridate provides a function for every possibility:
```{r}
ydm(x)
myd(x)
dmy(x)
dym(x)
```



Lubridate is also useful for dealing with times. In R, you can get the current time by typing `Sys.time()`. Lubridate provides a slightly more advanced function, `now`, that permits you define the time zone:

```{r}
now()
now("GMT")
```

You can see all the available times zones with the `OlsonNames()` function.

Lubridate also has a function to extract hours, minutes and seconds:

```{r}
now() %>% hour()
now() %>% minute()
now() %>% second()
```

as well as a function to convert strings into times:

```{r}
x <- c("12:34:56")
hms(x)
```

as well as parsers for time objects that include dates:

```{r}
x <- "Nov/2/2012 12:34:56"
mdy_hms(x)
```


## Web Scraping 

The data we need to answer a question is not always in a spreadsheet, ready for us to read. For example, the US murders dataset we used in the R Basics module originally comes from this Wikipedia page: [https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state](https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state). You can see the data table when you visit the web page.

But, unfortunately, there is no link to a data file. To make the data frame we loaded using `data(murders)`, or reading the csv file made available through `dslabs`, we had to do some _web scraping_. 

**_Web scraping_, or _web harvesting_, are the terms we use to describe the process of extracting data from a website**. The reason we can do this is because the information used by a browser to render web pages is received as **text** from a server. The text is computer code written in hyper text markup language (HTML). To see the code for a web page you can visit the page on your browser, then you can use the _View Source_ tool to see it.


Because this code is accessible, we can download the HTML files, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here we show a few lines of code from the Wikipedia page that provides the US murders data:

```{r, eval = FALSE}
p>The 2015 U.S. population total was 320.9 million. The 2015 U.S. overall murder rate per 100,000 inhabitants was 4.89.</p>
<h2><span class="mw-headline" id="States">States</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Murder_in_the_United_States_by_state&amp;action=edit&amp;section=1" title="Edit section: States">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="wikitable sortable">
<tr>
<th>State</th>
<th><a href="/wiki/List_of_U.S._states_and_territories_by_population" title="List of U.S. states and territories by population">Population</a><br />
<small>(total inhabitants)</small><br />
<small>(2015)</small> <sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup></th>
<th>Murders and Nonnegligent
<p>Manslaughter<br />
<small>(total deaths)</small><br />
<small>(2015)</small> <sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup></p>
</th>
<th>Murder and Nonnegligent
<p>Manslaughter Rate<br />
<small>(per 100,000 inhabitants)</small><br />
<small>(2015)</small></p>
</th>
</tr>
<tr>
<td><a href="/wiki/Alabama" title="Alabama">Alabama</a></td>
<td>4,853,875</td>
<td>348</td>
<td>7.2</td>
</tr>
<tr>
<td><a href="/wiki/Alaska" title="Alaska">Alaska</a></td>
<td>737,709</td>
<td>59</td>
<td>8.0</td>
</tr>
<tr>
```

You can actually see the data! We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make web pages look "pretty" called Cascading Style Sheets (CSS).

Although we provide tools that make it possible to scrape data without knowing HTML, for data scientists, it is quite useful to learn some HTML and CSS. Not only does this improve your scraping skills but it might come in handy if you are creating a webpage to showcase your work. There are plenty of online courses and tutorials for learning these. Two examples are [code academy](https://www.codecademy.com/learn/learn-html) and [WWW3 school](https://www.w3schools.com/)

### The `rvest` package

The `tidyverse` provides a web harvesting package called `rvest`. The first step in using this package is to import the web page into R. The package makes this quite simple:

```{r, message=FALSE, warning=FALSE}
library(rvest)
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
h   <- read_html(url)
```

Note that the entire Murders in the US Wikipedia webpage is now contained in `h`. The class of this object is

```{r}
class(h)
```

The `rvest` package is actually more general, it handles XML documents. XML is a general markup language, that's what the ML stands for, that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing web pages. Here we focus on HTML documents.

Now, how do we extract the table from the object `h`? if we print `h` we don't really see much:

```{r}
h
```

When we know that the information is stored in an HTML table, you can see this in this line of the HTML code above `<table class="wikitable sortable">`. For this we can use the following code. The different parts of an HTML document, often defined with a message in between  `<` and `>`  are referred to as _nodes_. The `rvest` package includes functions to extract nodes of an HTML document: `html_nodes` extracts all nodes of different type and `html_node` extracts the first one. To extract all tables we use:
 
```{r} 
tab <- h %>% html_nodes("table")
```

Now, instead of the entire web page, we just have the html code for the tables:

```{r}
tab
```

But we want the second table on the page since the first table is the legend that details what the colors mean. Looking at the output above it looks like the table index is [2]. To extract just the second table - the table with the data we are interetsed in - we can type the following:

```{r}
tab <- h %>% html_nodes("table") %>% .[2]
head(tab)
class(tab)
```


We are not quite there yet because this is clearly not a tidy dataset, not even a data frame. In the code above you can definitely see a pattern and writing code to extract just the data is very doable. In fact, `rvest` includes a function just for converting HTML tables into data frames:


```{r}
tab <- tab %>% html_table %>% .[[1]] 
class(tab)
```

We are now much closer to having a usable data table:

```{r}
tab <- tab %>% setNames(c("state", "population", "murder_manslaughter_total", "murder_total", "gun_murder_total", "ownership", "murder_manslaughter_rate",  "murder_rate", "gun_rate"))
head(tab)
```

We still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites.


### CSS Selectors

The default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS. CSS is used to add style to webpages. The fact that all pages for a company have the same style is usually a result that they all use the same CSS file. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links for example, each receive their own style including font, color, size, and distance from the margin, among others. To do this CSS leverages patterns used to define these elements, referred to as _selectors_. An example of a pattern we used above is `table` but there are many many more. 

So if we want to grab data from a web page and we happen to know a selector that is unique to the part of the page, we can use the `html_nodes` function. However, knowing which selector can be quite complicated. To demonstrate this we will try to extract the recipe name, total preparation time, and list of ingredients from [this](http://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609) guacamole recipe. Looking at the code for this page, it seems that the task is impossibly complex. However, selector gadgets actually make this possible.

[SelectorGadget](http://selectorgadget.com/) is piece of software that allows you to interactively determine what css selector you need to extract specific components from the web page. If you plan on scrapping data other than tables we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then as you click through the page it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this. 

For the guacamole recipe page we have already done this and determined that we need the following selectors:

```{r}
h           <- read_html("http://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609")
recipe      <- h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
prep_time   <- h %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients <- h %>% html_nodes(".o-Ingredients__a-Ingredient+ .o-Ingredients__a-Ingredient .o-Ingredients__a-Ingredient--CheckboxLabel") %>% html_text()
```

You can see how complex the selectors are. In any case, we are now ready to extract what we want and create a list:

```{r}
guacamole <- list(recipe, prep_time, ingredients)
guacamole
```

Since recipe pages from this website follow this general layout, we can use this code to create a function that extracts this information: 

```{r}
get_recipe <- function(url){
  h           <- read_html(url)
  recipe      <- h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
  prep_time   <- h %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
  ingredients <- h %>% html_nodes(".o-Ingredients__a-Ingredient+ .o-Ingredients__a-Ingredient .o-Ingredients__a-Ingredient--CheckboxLabel") %>% html_text()
  return(list(recipe = recipe, prep_time = prep_time, ingredients = ingredients))
}
```

and then use it on any of their webpages:

```{r}
get_recipe("http://www.foodnetwork.com/recipes/food-network-kitchen/pancakes-recipe-1913844")
```


There are several other powerful tools provided by `rvest`. For example the functions `html_form`, `set_values`, and `submit_form` permit you to query a web page from R. This is a more advanced topic not covered here.


